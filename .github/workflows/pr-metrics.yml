name: AI PR Summary with Cache
on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  pr-summary:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Create required assets directory
        run: |
          mkdir -p ./assets
          echo "# Touch this file to install the latest version of Ollama." > ./assets/version-file.txt
          
          # Format CORRECT pour models-file.txt (sans commentaires)
          echo "codellama:latest" > ./assets/models-file.txt
          
          # Prompt file
          cat << 'EOF' > ./assets/prompt-file.txt
          You are an expert code reviewer with deep knowledge of software development practices.
          You have been given the output of the git diff command, which shows the differences between the original and modified versions of a set of files.
          Please review these changes and provide a structured code review, starting with the following statement: 'This is the code review from AI'.
          Then, for each file, list the filename as a bullet point and describe the changes in that file as sub-bullets under the filename.
          Focus on explaining the purpose of the changes, potential improvements, best practices, and any potential issues.
          The review should be professional, concise, and informative.
          Answer as quickly as possible in English language.
          Here is the git diff output for review:
          EOF

      - name: Cache Ollama models
        uses: actions/cache@v4
        id: cache-models
        with:
          path: ~/.ollama/models
          key: ollama-models-${{ runner.os }}-${{ hashFiles('assets/models-file.txt') }}
          restore-keys: |
            ollama-models-${{ runner.os }}-

      - name: Install and start Ollama
        run: |
          # Installer Ollama
          curl -fsSL https://ollama.ai/install.sh | sh
          
          # Arrêter Ollama s'il est déjà en cours d'exécution
          pkill -f ollama || true
          sleep 2
          
          # Démarrer Ollama
          nohup ollama serve &
          sleep 15
          
          # Vérifier que le service fonctionne
          curl -s http://localhost:11434/api/tags || echo "Ollama starting..."

      - name: Pull AI models (if not cached)
        if: steps.cache-models.outputs.cache-hit != 'true'
        run: |
          # Attendre que le service soit complètement démarré
          until curl -s http://localhost:11434/api/tags > /dev/null 2>&1; do
            echo "Waiting for Ollama to start..."
            sleep 5
          done
          
          # Télécharger les modèles listés dans models-file.txt
          while IFS= read -r model; do
            # Ignorer les lignes vides et les commentaires
            if [[ -n "$model" && ! "$model" =~ ^# ]]; then
              echo "Pulling model: $model"
              ollama pull "$model"
            fi
          done < ./assets/models-file.txt

      - name: Summarize PR with AI
        uses: behrouz-rad/ai-pr-summarizer@v1
        with:
          llm-model: 'codellama:latest'
          prompt-file: ./assets/prompt-file.txt
          models-file: ./assets/models-file.txt
          version-file: ./assets/version-file.txt
          context-window: 4096
          upload-changes: true
          token: ${{ secrets.GITHUB_TOKEN }}